{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81dfd09a",
   "metadata": {},
   "source": [
    "## Portfolio Assignment week 01\n",
    "\n",
    "Study the Tutorial tutorial_cluster_scanpy_object and the tutorial_Clustering_Methods\n",
    "\n",
    "Write a brief summary about the following:\n",
    "\n",
    "-\tWhat are common preprocessing steps? Explain for each step why and when you should execute this step and when not.\n",
    "-\tWhat visualization methods are used in the cluster methods tutorial? Explain why the selected method is the most appropriate method for the visualization. Bonus points: do this as well for the scanpy tutorial.\n",
    "-\tWhat performance/evaluation metrics are in the cluster methods tutorial? Explain why the used methods are the most appropriate method for the evaluation.\n",
    "\n",
    "\n",
    "Bonus:\n",
    "You practice the steps yourself with the breast_cancer dataset (clustering_data.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a37c61",
   "metadata": {},
   "source": [
    "#### What are common preprocessing steps? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea250481",
   "metadata": {},
   "source": [
    "Common preprocessing steps are essential data preparation techniques used to clean, transform, and organize raw data into a format suitable for analysis or machine learning. Here are some of the most common preprocessing steps described bellow - \n",
    "\n",
    "Data Cleaning -  This step involves handling missing values, outliers, and inconsistent data. Missing values can be imputed or dropped, outliers can be detected and handled appropriately. Data cleaning ensures the quality and reliability of the dataset.\n",
    "\n",
    "Data Transformation - \n",
    "    Normalization: Normalization is applied when the features have different scales and need to be standardized. It transforms the data so that it has a specific distribution, often with zero mean and unit variance.\n",
    "    Standardization: Transform numerical features to have zero mean and unit variance.\n",
    "    Encoding categorical variables: Convert categorical variables into numerical representations (e.g., one-hot encoding or label encoding).\n",
    "    Feature scaling: Ensure that numerical features have similar scales to prevent dominance by a single feature.\n",
    "\n",
    "Date and Time Preprocessing - \n",
    "    Extracting components: Break down date and time features into year, month, day, etc.\n",
    "    Creating time-based features: Generate features like day of the week, time of day, or time since a specific event.\n",
    "\n",
    "Data Visualization - \n",
    "    Exploratory Data Analysis (EDA): Visualize and analyze data distributions, relationships, and patterns.\n",
    "\n",
    "    \n",
    "#### why and when you should execute this step and when not.\n",
    "\n",
    "Data quality: If the dataset contains missing values, outliers, or inconsistent data, data cleaning becomes necessary.\n",
    "\n",
    "Algorithm requirements: Some algorithms require feature scaling, while others are robust to feature scales.\n",
    "\n",
    "Dataset size: Feature selection or dimensionality reduction becomes more important when dealing with high-dimensional datasets to improve efficiency and prevent overfitting.\n",
    "\n",
    "Evaluation strategy: Train-test split or cross-validation helps evaluate the model's generalization performance on unseen data.\n",
    "\n",
    "However, all preprocessing steps are not mandatory in every situation. For example, if the dataset is clean and free of outliers, data cleaning may not be required. Similarly, if the algorithm used is not sensitive to feature scaling, it may not be necessary to perform feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0a0556",
   "metadata": {},
   "source": [
    "#### What visualization methods are used in the cluster methods tutorial? Explain why the selected method is the most appropriate method for the visualization. Bonus points: do this as well for the scanpy tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110f961d",
   "metadata": {},
   "source": [
    "The cluster methods tutorial uses a dendogram produced by agglomerative clustering.And it visualize the cluster in scRNA-seq data By examining the dendrogram, we can identify different levels of clustering granularity and explore subpopulations of cells at various levels.\n",
    "In the context of scRNA-seq (single-cell RNA sequencing) data analysis, agglomerative clustering is often used to explore the hierarchical structure of cell populations and their relationships. Agglomerative clustering is a bottom-up hierarchical clustering approach where individual data points (cells) are initially considered as separate clusters and then successively merged based on their similarity or distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bfb061",
   "metadata": {},
   "source": [
    "The cluster scanpy object tutorial uses UMAP for visualization. The UMAP (Uniform Manifold Approximation and Projection) plot is a commonly used visualization tool to explore the clustering results and gain insights into underlying cell populations.UMAP is a dimensionality reduction technique that aims to preserve the local structure of the data while reducing its dimensionality. It is particularly well-suited for visualizing scRNA-seq data because it can effectively capture nonlinear relationships and complex patterns in high-dimensional gene expression data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496d0d4f",
   "metadata": {},
   "source": [
    "#### What performance/evaluation metrics are in the cluster methods tutorial? Explain why the used methods are the most appropriate method for the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c1ce88",
   "metadata": {},
   "source": [
    "The silhouette score measures how close each data point in one cluster is to the points in the neighboring clusters. A higher silhouette score indicates better-defined clusters. It is calculated as the difference between the mean distance to the points in the same cluster and the mean distance to the points in the nearest neighboring cluster, divided by the maximum of the two.\n",
    "\n",
    "The choice of evaluation metric depends on the characteristics of the data and the goals of the clustering analysis. Different metrics emphasize different aspects of clustering quality, such as compactness, separation, or agreement with ground truth labels. The appropriate method for evaluation should align with the specific objectives of the analysis and provide insights into the effectiveness of the chosen clustering algorithm in discovering meaningful patterns or groups within the data. It's often recommended to use a combination of metrics and to consider domain knowledge when interpreting the results of cluster analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68ecfc7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
